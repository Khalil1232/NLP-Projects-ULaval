# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wmq9DO3FsQ3UH2cv5UCBGpV4XScmpjBf
"""

import torch
START_VOCAB = ["_UNK"]
UNK_ID = 0
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""#Création du jeu de données"""

#On charge nos données à partir du pc
from google.colab import files
uploaded=files.upload()

proverbs_fn = "proverbes.txt"
test1_fn = "test_proverbes.txt"

#On charge les données
import json
def load_proverbs(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        raw_lines = f.readlines()
    return [x.strip() for x in raw_lines]

def load_tests(filename):
    with open(filename, 'r') as fp:
        test_data = json.load(fp)
    return test_data

def create_vocabulary(corpus_path, vocab_path, max_vocab_size=1e5):
    """Créer et sauvegarder le vocabulaire d'un corpus."""
    vocab = {}
    with open(corpus_path, "r", encoding="utf-8") as f:
        for line in f:
            tokens = line.strip().split()
            for token in tokens:
                if token in vocab:
                    vocab[token] += 1
                else:
                    vocab[token] = 1
    vocab_list = START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)
    if len(vocab_list) > max_vocab_size:
        vocab_list = vocab_list[:max_vocab_size]
    with open(vocab_path, "w", encoding="utf-8") as f:
        for token in vocab_list:
              f.write(token + "\n")

import os
def initialize_vocabulary(vocab_path):
    """Initialiser le vocabulaire."""
    if os.path.exists(vocab_path):
        with open(vocab_path, "r", encoding="utf-8") as f:
            rev_vocab = [line.strip() for line in f.readlines()]
        vocab = dict([(w, i) for (i, w) in enumerate(rev_vocab)])
        return vocab, rev_vocab
    else:
        raise ValueError("Vocabulary file {} not found.".format(vocab_path))

def read_corpus(corpus_path):
    """Lire et convertir un corpus en une liste de tokens."""
    with open(corpus_path, "r", encoding="utf-8") as f:
      corpus = f.read().split()
    return corpus

def corpus_to_token_ids(corpus, vocab):
    """Convertir un corpus en token-ids."""
    token_ids = [vocab.get(token, UNK_ID) for token in corpus]
    return token_ids

import numpy as np
def batch_data(data, batch_size):
    """Structurer les données en batch_size séquences continues."""
    n_batch = len(data) // batch_size
    data = np.array(data[:n_batch*batch_size])
    data = data.reshape(batch_size, -1).T
    return data

def detach_hidden(hidden):
    """Transformer les données des états cachés d'un LSTM en
       nouveaux Tensor avec require_grad=False."""
    return tuple(h.detach() for h in hidden)

corpus_path =proverbs_fn
vocab_path = "vocab.txt"

create_vocabulary(corpus_path, vocab_path)
vocab, rev_vocab = initialize_vocabulary(vocab_path)

# Lire le corpus d'entraînement.
corpus = read_corpus(corpus_path)
token_ids = corpus_to_token_ids(corpus, vocab)

import numpy as np
# Structurer le corpus en batch_size séquences continues pour faire l'entraînement.
batch_size = 10
data = batch_data(token_ids, batch_size)
data = torch.LongTensor(data).to(DEVICE)

print("Nombre de tokens dans le corpus: {}"
      .format(len(corpus)), end="\n\n")
print("Taille du vocabulaire: {}"
      .format(len(vocab)), end="\n\n")
print("Liste des 20 tokens les plus fréquents dans le corpus: \n{}"
      .format(rev_vocab[1:21]), end="\n\n")
print("Première phrase du corpus en format texte:\n{}"
      .format(" ".join(corpus[:7])), end="\n\n")
print("Première phrase du corpus transformée en token-ids:\n{}"
      .format(token_ids[:7]), end="\n\n")
print("Test de conversion token-ids vers texte en utilisant rev_vocab:\n{}"
      .format(" ".join([rev_vocab[i] for i in token_ids[:31]])), end="\n\n")
print("Structure des données d'entraînement (observez que la première phrase "
      "est dans la première colonne):\n{}".format(data[:20]))

# Commented out IPython magic to ensure Python compatibility.
import copy
import time
import os

import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.optim as optim

from torch.utils.data import DataLoader, TensorDataset


use_gpu = torch.cuda.is_available()
device = torch.device("cuda:0" if use_gpu else "cpu")

# Seed pour reproduire les résultats.
seed = 42
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)

print("PyTorch version: ", torch.__version__)
print("GPU available: {}".format(use_gpu))

# Split ensembles training/validation et créer DataLoaders.
X = data[:-1]
Y = data[1:]

n_valid = round(data.size(0) * 0.1)
train_set = TensorDataset(X[:(data.size(0)-n_valid)], Y[:(data.size(0)-n_valid)])
valid_set = TensorDataset(X[-n_valid:], Y[-n_valid:])

seq_len = 40
train_loader = DataLoader(train_set, batch_size=seq_len, shuffle=False)
valid_loader = DataLoader(valid_set, batch_size=seq_len, shuffle=False)

print("Ensemble d'entraînement: {} séquences de longueur {} et {} minibatches"
      .format(len(train_set), seq_len, len(train_loader)))
print("Ensemble de validation : {} séquences de longueur {} et {} minibatches"
      .format(len(valid_set), seq_len, len(valid_loader)))

#Implémentation du modèle
class LanguageModel(nn.Module):
    """Modèle de langue neuronal à base de LSTM."""
    
    def __init__(self, vocab_size, input_size, hidden_size, n_layers=1, dropout=0.5):
        """
        Args:
          vocab_size: taille du vocabulaire.
          input_size: taille des word embeddings.
          hidden_size: taille des états cachés du LSTM.
          n_layers: nombre de couches du LSTM (default: 1).
          dropout: si non-zéro, introduit une couche dropout à l'entrée et à la sortie
                   du LSTM, avec une probabilité égale à dropout (default: 0.5).
        """        
        super(LanguageModel, self).__init__()
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        
        self.embeddings = nn.Embedding(vocab_size, input_size)
        self.lstm = nn.LSTM(input_size,
                            hidden_size,
                            n_layers)
        self.linear = nn.Linear(hidden_size, vocab_size)
        self.dropout = nn.Dropout(dropout)

        self.init_weights()

    def forward(self, input, hidden):
        embeddings = self.dropout(self.embeddings(input))
        output, hidden = self.lstm(embeddings, hidden)
        output = self.dropout(output)
        result = self.linear(output.view(output.size(0)*output.size(1), output.size(2)))
        return result.view(output.size(0), output.size(1), result.size(1)), hidden
    def init_weights(self):
        init.uniform_(self.embeddings.weight, -0.1, 0.1)
        init.xavier_uniform_(self.linear.weight, init.calculate_gain("linear"))
        init.constant_(self.linear.bias, 0)
        
    def init_hidden(self, batch_size):
        """Initialiser les valeurs de l'état caché et la cellule du LSTM à zéro.
        Args:
          batch_size: taille de la mini-batch à un pas de temps.
          
        Returns:
          hidden: état caché h_t et la cellule c_t à t=0 initialisés à 0, 
                  ((n_layers, batch_size, hidden_size),
                   (n_layers, batch_size, hidden_size)).
        """
        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_size, device=DEVICE),
                  torch.zeros(self.n_layers, batch_size, self.hidden_size, device=DEVICE))
        return hidden

#Entraînement du modèle
# Construire le modèle.
vocab_size = len(vocab)
embedding_size = 300
hidden_size = 400
n_layers = 1
dropout = 0.65
model = LanguageModel(vocab_size, embedding_size, hidden_size, n_layers, dropout).to(DEVICE)
print("Nombre de paramètres dans le modèle:", sum(param.nelement() for param in model.parameters()))

# Fonction de coût et optimiseur.
learning_rate = 10
loss_fun = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2, verbose=True)

# Entraînement du modèle.
n_epochs = 20
max_grad_norm = 1
print("Entraînement du modèle pour {} epochs de {} minibatches".format(n_epochs, len(train_loader)))
for epoch in range(n_epochs):
    model.train()
    hidden = model.init_hidden(batch_size)
    train_loss = 0
    valid_loss = 0
    for x, y in train_loader:
        # Détacher les états cachés précédents du graphe computationnel.
        hidden = detach_hidden(hidden)
        
        # Réinitialiser le gradient.
        optimizer.zero_grad()
                
        # Forward pass.
        y_pred, hidden = model(x, hidden)
        
        # Calculer la perte.
        loss = loss_fun(y_pred.view(-1, vocab_size), y.view(-1))
        
        # Calculer le gradient.
        loss.backward()
        
        # Normaliser le gradient pour éviter explosion.
        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        
        # Mettre à jour les paramètres du modèle.
        optimizer.step()        
        
        # Accumuler la perte total.
        train_loss += len(x) * loss.item()
    # Évaluer le modèle sur l'ensemble de validation.
    model.eval()
    with torch.no_grad():
        hidden = model.init_hidden(batch_size)
        for x, y in valid_loader:
            y_pred, hidden = model(x, hidden)
            loss = loss_fun(y_pred.view(-1, vocab_size), y.view(-1))
            valid_loss += len(x) * loss.item()
    
    train_loss /= len(train_loader.dataset)
    valid_loss /= len(valid_loader.dataset)
    scheduler.step(valid_loss)

    print("Epoch {:2d} | Training loss = {:.5f} | Validation loss = {:.5f} | Perplexity = {:.2f}"
          .format(epoch+1, train_loss, valid_loss, np.exp(valid_loss)))
print("Félicitations! Vous avez terminé d'entraîner votre beau modèle de langue neuronal!")

#On fait le test sur nos données de test
corpus_path_test=test1_fn
corpus_test= read_corpus(corpus_path_test)
#print(corpus_test)
token_ids_test = corpus_to_token_ids(corpus_test, vocab)
print(token_ids_test)

data_test = batch_data(token_ids_test, batch_size)
data_test = torch.LongTensor(data_test).to(DEVICE)
A = data_test[:-1]
B = data_test[1:]
test_set = TensorDataset(A[:data_test.size(0)], B[:data_test.size(0)])

test_loader = DataLoader(test_set, batch_size=seq_len, shuffle=False)
print("Ensemble de test: {} séquences de longueur {} et {} minibatches".format(len(test_set), seq_len, len(test_loader)))

test_acc=0
i=0
for epoch in range(n_epochs):
  model.train()
  hidden = model.init_hidden(batch_size)
  test_loss = 0
  #valid_loss = 0
  # Évaluer le modèle sur l'ensemble de validation.
  model.eval()
  with torch.no_grad():
      hidden = model.init_hidden(batch_size)
      for x, y in test_loader:
          y_pred, hidden = model(x, hidden)
          loss = loss_fun(y_pred.view(-1, vocab_size), y.view(-1))
          test_loss += len(x) * loss.item()
    
  test_loss /= len(train_loader.dataset)
  scheduler.step(test_loss)
  test_acc=test_acc+test_loss
  print(test_acc)
  i=i+1
  print("Epoch {:2d} | Test loss = {:.5f} | score={:.5f}| Perplexity = {:.2f}"
          .format(epoch+1, test_loss,(1-test_loss)*100, np.exp(test_loss)))
test_acc=test_acc/i
print("la performance est de:", (1-test_acc)*100, "%")









!python3 -m spacy download en_core_web_md

import numpy as np
import spacy 
nlp = spacy.load('en_core_web_md')
from numpy import argmax

def obtain_prediction(sentence, label=None):
    tokens = [word.text for word in nlp(sentence)]
    sequence = [word2id.get(token, 1) for token in tokens]
    sentence_length = len(sequence)
    class_scores = model(LongTensor(sequence).unsqueeze(0), LongTensor([sentence_length])).detach().numpy()
    best_score_id = argmax(class_scores)
    return id2label[best_score_id]









# Génération de séquences de mots.
model.eval()
x = torch.randint(0, vocab_size, (1, 1), dtype=torch.long, device=DEVICE)
words = [rev_vocab[x]]
n_words = 300
smoothing = 0.7
with torch.no_grad(): 
    hidden = model.init_hidden(1)
    for i in range(n_words-1):
        output, hidden = model(x, hidden)
        weights = output.squeeze().div(smoothing).exp()
        word_idx = torch.multinomial(weights, 1)
        x.fill_(word_idx.squeeze())
        word = rev_vocab[word_idx]
        words.append(word)
        if (i+1) % 15 == 0:
            words.append("\n")
print(" ".join(words))