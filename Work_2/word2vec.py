# -*- coding: utf-8 -*-
"""word2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Kz2ektkB0F-gChmRSe2jzYHnS1cMJxR
"""

#On charge nos données à partir du pc
from google.colab import files
uploaded=files.upload()

"""#1- Création du jeu de données"""

training_questions_fn = "questions-train.txt"
test_questions_fn = "questions-test.txt"

def load_dataset(filename):
    with open(filename) as f:
        lines = f.read().splitlines()
        labels, questions = zip(*[tuple(s.split(' ', 1)) for s in lines])
    return questions, labels

#On charge nos données
questions_train, labels_train = load_dataset(training_questions_fn)
questions_test, labels_test = load_dataset(test_questions_fn)

#Quelques informations sur le jeu de données
print("Pour l'ensemble d'entrainement, on a: {} exemples de catégories {}.".format(len(questions_train), set(labels_train)))

print("Pour l'ensemble de test, on a: {} exemples de catégories {}.".format(len(questions_test), set(labels_test)))

#On convertit nos classes en identifiants
label_list = list(set(labels_train))
id2label = {label_id: value for label_id, value in enumerate(label_list)}
label2id = {value: label_id for label_id, value in enumerate(label_list)}
nb_classes = len(label_list)

print(label2id)
target_categories=list(label2id.keys())
print(target_categories)

y = [label2id[label] for label in labels_train]
print(y)

#Finalement, on divise notre ensemble d'entraînement en jeu d'entraînement (90%) et de validation (10%)
from sklearn.model_selection import train_test_split

X_train, X_valid, y_train, y_valid = train_test_split(questions_train, y, test_size=0.1, shuffle=True,random_state=42)

y_test = [label2id[label] for label in labels_test]
print(y_test)

print("Nb exemples d'entraînement: {}, Nb d'exemples de validation: {}.".format(len(y_train), len(y_valid)))

"""# 2- Gestion de plongements de mot avec word2vec"""

#Utilisation de word2vec par l'entremise de gensim
import gensim.downloader as api


pretrained_embeddings = {
    'word2vec300': 'word2vec-google-news-300'
}


def load_embeddings(embeddings_type):
    embeddings_fn = pretrained_embeddings[embeddings_type]
    return api.load(embeddings_fn)

embeddings_model = load_embeddings("word2vec300")

print("Infos sur le modèle de plongements préentraînés:", embeddings_model)



word = "eat"
pretrained_embedding = embeddings_model[word]
print("Taille du vecteur de plongement de mot:", pretrained_embedding.shape)
print("\nPlongement du mot {}:\n{}".format(word, pretrained_embedding) )

oov_word = 'xyxx'

try:
    print(embeddings_model[oov_word])
except KeyError:
    print("Mot inconnu - {} - n'est pas dans le vocabulaire de la table de plongements préentraînés".format(oov_word))







"""# 3- Création d'un reseau neuronal multicouche"""



#Création d'un réseau neuronal multicouches
from torch import nn


class MultiLayerPerceptron(nn.Module):
    
    def __init__(self, input_size, hidden_layer_size, output_size) :
        super().__init__()
        self.intput_layer = nn.Linear(input_size, hidden_layer_size)
        self.output_layer = nn.Linear(hidden_layer_size, output_size)
        
    def forward(self, x):
        x = self.intput_layer(x)
        x = nn.functional.relu_(x)
        x = self.output_layer(x)
        return x

"""# 4-Création d'un plongement de document à partir des plongements de ses mots"""

!python3 -m spacy download en_core_web_md

import numpy as np
import spacy 
nlp = spacy.load('en_core_web_md')
def average_embedding(sentence, nlp_model=nlp):
    tokenised_sentence = nlp_model(sentence)  # tokenized
    nb_column = len(tokenised_sentence)
    nb_rows =  nlp_model.vocab.vectors_length
    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                  
    for index, token in enumerate(tokenised_sentence):
        token=str(token)
        try:
          sentence_embedding_matrix[:, index] = embeddings_model[token]
        except KeyError:
          continue #print("Mot inconnu - {} - n'est pas dans le vocabulaire de la table de plongements préentraînés".format(token))
    return np.average(sentence_embedding_matrix, axis=1)

def maxpool_embedding(sentence, nlp_model=nlp): 
    tokenised_sentence = nlp_model(sentence)
    nb_column = len(tokenised_sentence)
    nb_rows =  nlp_model.vocab.vectors_length 
    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                    
    for index, token in enumerate(tokenised_sentence):
        token=str(token)
        try:
          sentence_embedding_matrix[:, index] = embeddings_model[token]
        except KeyError:
          continue #print("Mot inconnu - {} - n'est pas dans le vocabulaire de la table de plongements préentraînés".format(token))
    return np.max(sentence_embedding_matrix, axis=1)

"""# 5-Création d'une classe de Dataloader pour itérer sur les données en "minibatch"
"""

import numpy as np
from torch.utils.data import Dataset, DataLoader
from torch import FloatTensor, LongTensor
from typing import List

class Word2vecDataset(Dataset):
    def __init__(self, dataset: List[str] , target: np.array, sentence_aggregation_function):
        self.dataset = dataset
        self.doc_embeddings = [None for _ in range(len(dataset))]
        self.sentence_aggregation_function = sentence_aggregation_function 
        self.target = target
    
    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index):
        if self.doc_embeddings[index] is None:
            self.doc_embeddings[index] = self.sentence_aggregation_function(self.dataset[index])  
        return FloatTensor(self.doc_embeddings[index]), LongTensor([self.target[index]]).squeeze(0)



# Un dictionnaire pour choisir le type d'agrégation
aggregation = {
    "average" : average_embedding,
    "maxpool" : maxpool_embedding
}
test_texts=questions_test
test_targets=y_test
# On détermine ici comment la classe SpacyDataset construit la représentation d'un texte
# par l'agrégation des représentations de mots. 
# Choix possibles: "average", "maxpool", "spacy"
aggregation_function = aggregation["average"]  

# On finalise la construction des 3 jeux de données et leurs dataloaders
train_dataset = Word2vecDataset(X_train, y_train, aggregation_function)
valid_dataset = Word2vecDataset(X_valid, y_valid, aggregation_function)
test_dataset = Word2vecDataset(test_texts, test_targets, aggregation_function) # à changer

train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)
valid_dataloader = DataLoader(valid_dataset, batch_size=10, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True)

"""# 6-Création d'une boucle d'entraînement"""

embedding_size = nlp.meta['vectors']['width'] # La dimension des vecteurs d'embeddings de Spacy
nb_classes = 9

print("Taille des plongements de Spacy:", embedding_size)
print("Nombre de classes:", nb_classes)

!pip install poutyne

from poutyne.framework import Experiment
from poutyne import set_seeds
from torch.optim import SGD
import numpy as np

set_seeds(42)
hidden_size = 100

directory_name = 'model/{}_mlp'.format(aggregation_function.__name__)  

model = MultiLayerPerceptron(embedding_size, hidden_size, nb_classes)
experiment = Experiment(directory_name, 
                        model, 
                        optimizer = "SGD", 
                        task="classification")
model

logging = experiment.train(train_dataloader, valid_dataloader, epochs=150, disable_tensorboard=True)

"""# 7-Prédiction sur quelques exemples"""

#target_categories=['QUANTITY', 'DESCRIPTION', 'LOCATION', 'TEMPORAL', 'ENTITY', 'ORGANIZATION', 'ABBREVIATION', 'DEFINITION', 'PERSON']
from torch.nn.functional import softmax

def get_most_probable_class(sentence, model):
    vectorized_sentence = aggregation_function(sentence)
    prediction = model(FloatTensor(vectorized_sentence).squeeze(0)).detach()
    output = softmax(prediction, dim=0)
    max_category_index = np.argmax(output)
    max_category = target_categories[max_category_index]
    print("\nClassification de la phrase: ", sentence)
    print("Sorties du réseau de neurones:", prediction)
    print("Valeurs obtenues après application de softmax:", output)
    print("Meilleure classe: {} qui correspond en sortie à la neurone {}".format(max_category, max_category_index))
    return(max_category)

test_docs = ['How far is it from Denver to Aspen ?', 
             'What county is Modesto , California in ?',
             'Who was Galileo ?', 
             'What is an atom ?', 'When did Idaho become a state ?',
             'What metal has the highest melting point ?']

[get_most_probable_class(sentence, model) for sentence in test_docs]

"""# 8-Évaluation du modèle sur l'ensemble de test"""

experiment.test(test_dataloader)