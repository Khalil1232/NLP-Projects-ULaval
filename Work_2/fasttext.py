# -*- coding: utf-8 -*-
"""fasttext.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13sR6AWGDzqOQA4kcGJXrHzyG-hYFNid4
"""

#On charge nos données à partir du pc
from google.colab import files
uploaded=files.upload()

"""# Creation du jeu de données"""

training_questions_fn = "questions-train.txt"
test_questions_fn = "questions-test.txt"

def load_dataset(filename):
    with open(filename) as f:
        lines = f.read().splitlines()
        labels, questions = zip(*[tuple(s.split(' ', 1)) for s in lines])
    return questions, labels

#On charge nos données
questions_train, labels_train = load_dataset(training_questions_fn)
questions_test, labels_test = load_dataset(test_questions_fn)

#On affiche quelques informations
print("Pour l'ensemble d'entrainement, on a: {} exemples de catégories {}.".format(len(questions_train), set(labels_train)))

print("Pour l'ensemble de test, on a: {} exemples de catégories {}.".format(len(questions_test), set(labels_test)))

#On convertit nos classes en identifiants
label_list = list(set(labels_train))
id2label = {label_id: value for label_id, value in enumerate(label_list)}
label2id = {value: label_id for label_id, value in enumerate(label_list)}
nb_classes = len(label_list)

print(label2id)
target_categories=list(label2id.keys())
print(target_categories)

y = [label2id[label] for label in labels_train]
print(y)

y_test = [label2id[label] for label in labels_test]
print(y_test)



#Finalement, on divise notre ensemble d'entraînement en jeu d'entraînement (90%) et de validation (10%)
from sklearn.model_selection import train_test_split

X_train, X_valid, y_train, y_valid = train_test_split(questions_train, y, test_size=0.1, shuffle=True,random_state=42)

print("Nb exemples d'entraînement: {}, Nb d'exemples de validation: {}.".format(len(y_train), len(y_valid)))

"""# Gestion du plongement de mot avec wfasttext"""

!pip install fasttext

import fasttext.util

fasttext.util.download_model('en', if_exists='ignore')

fasttext_model = fasttext.load_model('cc.en.300.bin')

print("Infos sur le modèle de plongements préentraînés:", fasttext_model)

print("Nombre de mots dans le modèle:", len(fasttext_model.get_words()))

word = "eat"
pretrained_embedding = fasttext_model.get_word_vector(word)  # équivalent à fasttext_model[word]
print("Taille du vecteur de plongement de mot:", pretrained_embedding.shape)
print("\nPlongement du mot {}:\n{}".format(word, pretrained_embedding) )

oov_word = 'kaba'

try:
    print("FastText a construit le plongement suivant pour le mot {}:\n".format(oov_word))
    print(fasttext_model[oov_word])
except KeyError:
    print("Mot inconnu - {} - ça ne devrait pas se produire avec fastText".format(oov_word))

"""# Création d'un réseau neuronal multicouches"""

#Création d'un réseau neuronal multicouches
from torch import nn


class MultiLayerPerceptron(nn.Module):
    
    def __init__(self, input_size, hidden_layer_size, output_size) :
        super().__init__()
        self.intput_layer = nn.Linear(input_size, hidden_layer_size)
        self.output_layer = nn.Linear(hidden_layer_size, output_size)
        
    def forward(self, x):
        x = self.intput_layer(x)
        x = nn.functional.relu_(x)
        x = self.output_layer(x)
        return x

"""# 4-Création d'un plongement de document à partir des plongements de ses mots"""

!python3 -m spacy download en_core_web_md

import numpy as np
import spacy 
nlp = spacy.load('en_core_web_md')
def average_embedding(sentence, nlp_model=nlp):
    tokenised_sentence = nlp_model(sentence)  # tokenized
    nb_column = len(tokenised_sentence)
    nb_rows =  nlp_model.vocab.vectors_length
    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                  
    for index, token in enumerate(tokenised_sentence):
        token=str(token)
        try:
            sentence_embedding_matrix[:, index] = fasttext_model.get_word_vector(token)
        except KeyError:
          #continue
            print("Mot inconnu - {} - ça ne devrait pas se produire avec fastText".format(oov_word))
    return np.average(sentence_embedding_matrix, axis=1)

def maxpool_embedding(sentence, nlp_model=nlp): 
    tokenised_sentence = nlp_model(sentence)
    nb_column = len(tokenised_sentence)
    nb_rows =  nlp_model.vocab.vectors_length 
    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))                                    
    for index, token in enumerate(tokenised_sentence):
        token=str(token)
        try:
            sentence_embedding_matrix[:, index] = fasttext_model.get_word_vector(token)
        except KeyError:
          #continue
            print("Mot inconnu - {} - ça ne devrait pas se produire avec fastText".format(oov_word))
    return np.max(sentence_embedding_matrix, axis=1)





"""# 5-Création d'une classe de Dataloader pour itérer sur les données en "minibatch"
"""

from torch.utils.data import Dataset, DataLoader
from torch import FloatTensor, LongTensor
from typing import List

class FasttextDataset(Dataset):
    def __init__(self, dataset: List[str] , target: np.array, sentence_aggregation_function):
        self.dataset = dataset
        self.doc_embeddings = [None for _ in range(len(dataset))]
        self.sentence_aggregation_function = sentence_aggregation_function 
        self.target = target
    
    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index):
        if self.doc_embeddings[index] is None:
            self.doc_embeddings[index] = self.sentence_aggregation_function(self.dataset[index])  
        return FloatTensor(self.doc_embeddings[index]), LongTensor([self.target[index]]).squeeze(0)

# Un dictionnaire pour choisir le type d'agrégation
aggregation = {
    "average" : average_embedding,
    "maxpool" : maxpool_embedding
}
test_texts=questions_test
test_targets=y_test
# On détermine ici comment la classe SpacyDataset construit la représentation d'un texte
# par l'agrégation des représentations de mots. 
# Choix possibles: "average", "maxpool", "spacy"
aggregation_function = aggregation["average"]  

# On finalise la construction des 3 jeux de données et leurs dataloaders
train_dataset = FasttextDataset(X_train, y_train, aggregation_function)
valid_dataset = FasttextDataset(X_valid, y_valid, aggregation_function)
test_dataset = FasttextDataset(test_texts, test_targets, aggregation_function) # à changer

train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)
valid_dataloader = DataLoader(valid_dataset, batch_size=10, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True)

"""# 6-Création d'une boucle d'entraînement"""

embedding_size = nlp.meta['vectors']['width'] # La dimension des vecteurs d'embeddings de Spacy
nb_classes = 9

print("Taille des plongements de Spacy:", embedding_size)
print("Nombre de classes:", nb_classes)

!pip install poutyne

from poutyne.framework import Experiment
from poutyne import set_seeds
from torch.optim import SGD
import numpy as np

set_seeds(42)
hidden_size = 100

directory_name = 'model/{}_mlp'.format(aggregation_function.__name__)  

model = MultiLayerPerceptron(embedding_size, hidden_size, nb_classes)
experiment = Experiment(directory_name, 
                        model, 
                        optimizer = "SGD", 
                        task="classification")
model

logging = experiment.train(train_dataloader, valid_dataloader, epochs=150, disable_tensorboard=True)

"""# 7-Prédiction sur quelques exemples"""

from torch.nn.functional import softmax

def get_most_probable_class(sentence, model):
    vectorized_sentence = aggregation_function(sentence)
    prediction = model(FloatTensor(vectorized_sentence).squeeze(0)).detach()
    output = softmax(prediction, dim=0)
    max_category_index = np.argmax(output)
    max_category = target_categories[max_category_index]
    print("\nClassification de la phrase: ", sentence)
    print("Sorties du réseau de neurones:", prediction)
    print("Valeurs obtenues après application de softmax:", output)
    print("Meilleure classe: {} qui correspond en sortie à la neurone {}".format(max_category, max_category_index))
    return(max_category)

test_docs = ['How far is it from Denver to Aspen ?', 
             'What county is Modesto , California in ?',
             'Who was Galileo ?', 
             'What is an atom ?', 'When did Idaho become a state ?',
             'What metal has the highest melting point ?']

[get_most_probable_class(sentence, model) for sentence in test_docs]

"""# 8-Évaluation du modèle sur l'ensemble de test"""

experiment.test(test_dataloader)

!pip install fasttext













!python3 -m spacy download en_core_web_md



